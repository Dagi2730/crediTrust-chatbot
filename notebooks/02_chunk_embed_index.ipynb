{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4184494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Imports\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "# üìÅ Paths and directory setup\n",
    "# Example if the file is in a folder named 'data' inside your project folder:\n",
    "DATA_PATH = \"../data/filtered_complaints.csv\"\n",
    "VECTOR_STORE_DIR = \"./vector_store/\"\n",
    "os.makedirs(VECTOR_STORE_DIR, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d65b434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 344308 complaints.\n",
      "Prepared 344308 documents for chunking.\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned complaints CSV into a DataFrame\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"Loaded {len(df)} complaints.\")\n",
    "\n",
    "# Prepare a list of documents with relevant fields (ID, product, text)\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    text = str(row['Cleaned Narrative']) if pd.notna(row['Cleaned Narrative']) else \"\"\n",
    "    documents.append({\n",
    "        \"id\": row['Complaint ID'],\n",
    "        \"product\": row['Product'],\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(documents)} documents for chunking.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fef4ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered documents: 988\n"
     ]
    }
   ],
   "source": [
    "# Limit to first 1000 documents and remove short text\n",
    "documents_subset = [doc for doc in documents[:1000] if len(doc[\"text\"].strip()) > 100]\n",
    "print(f\"Filtered documents: {len(documents_subset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "263bf84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 989 with chunk_size=10000\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 10000  \n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "chunked_docs = []\n",
    "\n",
    "for doc in documents_subset:\n",
    "    chunks = text_splitter.split_text(doc['text'])\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunked_docs.append({\n",
    "            'id': doc['id'],\n",
    "            'product': doc['product'],\n",
    "            'chunk_text': chunk,\n",
    "            'chunk_index': i\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_docs)} with chunk_size={chunk_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84fda4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating embedding time...\n",
      "Total chunks: 989 ‚è±Ô∏è Estimated time: 0.41 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating embedding time...\")\n",
    "chunk_count = len(chunked_docs)\n",
    "avg_per_chunk = 0.025  # ~25ms per chunk (MiniLM L6)\n",
    "\n",
    "estimated_time_min = (chunk_count * avg_per_chunk) / 60\n",
    "print(f\"Total chunks: {chunk_count} ‚è±Ô∏è Estimated time: {estimated_time_min:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e99b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", local_files_only=True)\n",
    "print(f\"Loaded embedding model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "529dceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ca17ee32354f97950862fc4a4f7a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated and normalized embeddings for 989 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Extract texts from the chunked documents\n",
    "chunk_texts = [doc[\"chunk_text\"] for doc in chunked_docs]\n",
    "\n",
    "# Encode chunks with progress bar and batching\n",
    "embeddings = embedding_model.encode(\n",
    "    chunk_texts,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# Convert to float32 NumPy array\n",
    "embeddings = np.array(embeddings).astype(\"float32\")\n",
    "\n",
    "# Normalize embeddings for cosine similarity\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "print(f\"Generated and normalized embeddings for {len(embeddings)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "10bbef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and populated with 989 vectors.\n"
     ]
    }
   ],
   "source": [
    "# Get embedding dimension (e.g., 384 for MiniLM-L6-v2)\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Create FAISS index for cosine similarity (via inner product)\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "\n",
    "# Add all chunk embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"FAISS index created and populated with {index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa24df97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FAISS index to: ./vector_store/faiss_index.bin\n",
      "Saved chunk metadata to: ./vector_store/chunked_docs.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the FAISS index to disk\n",
    "faiss_index_path = os.path.join(VECTOR_STORE_DIR, \"faiss_index.bin\")\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "# Save the chunked metadata (so you can trace results later)\n",
    "chunk_metadata_path = os.path.join(VECTOR_STORE_DIR, \"chunked_docs.pkl\")\n",
    "with open(chunk_metadata_path, \"wb\") as f:\n",
    "    pickle.dump(chunked_docs, f)\n",
    "\n",
    "print(f\"Saved FAISS index to: {faiss_index_path}\")\n",
    "print(f\"Saved chunk metadata to: {chunk_metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a91efe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_docs is a list of dictionaries, convert it to a DataFrame and save as CSV\n",
    "\n",
    "pd.DataFrame(chunked_docs).to_csv(os.path.join(VECTOR_STORE_DIR, \"chunked_metadata.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b440eec",
   "metadata": {},
   "source": [
    "# Semantic Search with Text Chunking, Embedding, and FAISS Vector Index\n",
    "\n",
    "This code demonstrates how to perform efficient semantic search over a large collection of text complaints by:\n",
    "\n",
    "1. **Loading the Pre-built Vector Store and Metadata**  \n",
    "   - The FAISS vector index (`faiss_index.bin`) containing vector embeddings of text chunks is loaded.  \n",
    "   - The metadata for each chunk (complaint ID, product category, chunk text, etc.) is loaded from a saved pickle file and converted into a DataFrame for easy access.\n",
    "\n",
    "2. **Loading the Embedding Model**  \n",
    "   - We load the pre-trained `sentence-transformers/all-MiniLM-L6-v2` model, which is a lightweight and effective model for encoding text into dense vector representations suitable for semantic similarity tasks.\n",
    "\n",
    "3. **Defining the Semantic Search Function**  \n",
    "   - Given a user query (a short text), the function encodes it into a vector using the embedding model.  \n",
    "   - The query vector is normalized to match the FAISS index's format.  \n",
    "   - The FAISS index is queried to find the top-k most similar text chunks by comparing vector distances.  \n",
    "   - For each closest match, the function retrieves the corresponding metadata, including the original complaint ID, product category, chunk index, and a snippet of the chunk text, along with the similarity score.\n",
    "\n",
    "4. **Running a Sample Query**  \n",
    "   - We run a sample query (\"fraudulent charge on credit card\") to demonstrate how the semantic search returns relevant complaint chunks.  \n",
    "   - The top results are printed with key metadata and a snippet of the matching text.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- This pipeline enables fast and scalable semantic search on large text corpora by splitting long narratives into manageable chunks, embedding them into vector space, and indexing with FAISS for quick similarity retrieval.  \n",
    "- Metadata stored alongside embeddings ensures that search results can be traced back to their original source documents for interpretability.  \n",
    "- The sentence-transformers model provides a balance of speed and accuracy for embedding generation, making it suitable for production-scale applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e715e7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Directory where vector store and metadata are saved\n",
    "VECTOR_STORE_DIR = \"../vector_store/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c86f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 989 vectors.\n"
     ]
    }
   ],
   "source": [
    "faiss_index_path = os.path.join(VECTOR_STORE_DIR, \"faiss_index.bin\")\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "print(f\"Loaded FAISS index with {index.ntotal} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "59927f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded metadata with 989 chunks.\n"
     ]
    }
   ],
   "source": [
    "metadata_path = os.path.join(VECTOR_STORE_DIR, \"chunked_docs.pkl\")\n",
    "\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    chunked_docs = pickle.load(f)\n",
    "\n",
    "metadata_df = pd.DataFrame(chunked_docs)\n",
    "print(f\"Loaded metadata with {len(metadata_df)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d4e4335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "print(f\"Loaded embedding model: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7c2fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=5):\n",
    "    # Embed the query to a vector\n",
    "    query_vec = embedding_model.encode([query])\n",
    "    query_vec = np.array(query_vec).astype(\"float32\")\n",
    "    \n",
    "    # Normalize the query vector (since FAISS index is normalized)\n",
    "    faiss.normalize_L2(query_vec)\n",
    "    \n",
    "    # Search the FAISS index for the closest vectors\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        row = metadata_df.iloc[idx]\n",
    "        results.append({\n",
    "            \"id\": row['id'],\n",
    "            \"product\": row['product'],\n",
    "            \"chunk_index\": row['chunk_index'],\n",
    "            \"chunk_text\": row['chunk_text'],\n",
    "            \"score\": dist\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61969485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 results for query: 'fraudulent charge on credit card'\n",
      "\n",
      "1. Complaint ID: 13885761, Product: Credit card, Chunk index: 0, Similarity score: 0.7013\n",
      "   Text snippet: on xxxxyear i made rented a car in the xxxx xxxx company name xxxx xxxx xxxx originally i was going to put the charge of my card but they offered a substantial discount if i paid in cash they informed...\n",
      "\n",
      "2. Complaint ID: 13993418, Product: Credit card, Chunk index: 0, Similarity score: 0.6777\n",
      "   Text snippet: xxxx credit card has charged me exponentially more than what my original borrow request was for after trying to resolve the situation i informed them that if they did not remove the outrageous interes...\n",
      "\n",
      "3. Complaint ID: 13575993, Product: Credit card, Chunk index: 0, Similarity score: 0.6647\n",
      "   Text snippet: on xxxxscrub 2024 i incurred a fraudulent charge on my citibank xxxx xxxx xxxx ending in xxxx for 91000 from xxxx xxxx the fraud was immediately reported and a new card was issued despite my numerous ...\n",
      "\n",
      "4. Complaint ID: 13640748, Product: Credit card, Chunk index: 0, Similarity score: 0.6552\n",
      "   Text snippet: i had a charge of xxxx on my card for vitamins that i definitely did not order called the revvi credit company and made a fraud complaint 2 weeks to the day later a charge of xxxx can not remember exa...\n",
      "\n",
      "5. Complaint ID: 13859509, Product: Credit card, Chunk index: 0, Similarity score: 0.6538\n",
      "   Text snippet: i recieved a pre approved card in the mail i activated the card the card was loststolen the card was used and maxed out i got a notification that the balance was maxed out i called to close card and r...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"fraudulent charge on credit card\"\n",
    "results = semantic_search(query)\n",
    "\n",
    "print(f\"Top {len(results)} results for query: '{query}'\\n\")\n",
    "\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"{i}. Complaint ID: {res['id']}, Product: {res['product']}, Chunk index: {res['chunk_index']}, Similarity score: {res['score']:.4f}\")\n",
    "    print(f\"   Text snippet: {res['chunk_text'][:200]}...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
